{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Machine Learning practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for investigation  \n",
    "  \n",
    "  We'll be exploring the netflix-rotten-tomatoes-metacritic-imdb.csv dataset we were working with last week.  \n",
    "    \n",
    "We know that our client is interested in winning awards, and we've tried analysing the data to find patterns using simple relationships between the variables.   \n",
    "\n",
    "Now we're going to see if machine learning can come up with any insights.  \n",
    "\n",
    "1. What factors most affect the number of awards won by a movie?  \n",
    "2. Of those factors, which carry the most weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules we will need to use \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning models and utilities \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "# synthetic dataset for classification (binary) \n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# more difficult synthetic dataset for classification (binary) \n",
    "# with classes that are not linearly separable\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_class_knn(X, y, n_neighbors, weights, X_test, y_test):\n",
    "    from sklearn import neighbors\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    X_mat = X\n",
    "    y_mat = y\n",
    "\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFFFAA', '#AAFFAA', '#AAAAFF','#EFEFEF'])\n",
    "    cmap_bold  = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X_mat, y_mat)\n",
    "\n",
    "    # Plot the decision boundary by assigning a color in the color map\n",
    "    # to each mesh point.\n",
    "\n",
    "    mesh_step_size = .01  # step size in the mesh\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n",
    "    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),\n",
    "                         np.arange(y_min, y_max, mesh_step_size))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n",
    "\n",
    "    # Plot training points\n",
    "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    title = \"Neighbors = {}\".format(n_neighbors)\n",
    "    if (X_test is not None):\n",
    "        train_score = clf.score(X_mat, y_mat)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    patch0 = mpatches.Patch(color='#FFFF00', label='class 0')\n",
    "    patch1 = mpatches.Patch(color='#000000', label='class 1')\n",
    "    plt.legend(handles=[patch0, patch1])\n",
    "\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'.format(linreg.coef_)) # weighting/slope\n",
    "print('linear model intercept (b): {:.3f}'.format(linreg.intercept_)) # intercept\n",
    "print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train))) # error on train\n",
    "print('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test))) # error on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression: example plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The movies dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching and shaping our data \n",
    "  \n",
    "Different machine learning models need the input data shaped in specific ways.  \n",
    "In general, they need:\n",
    "* Numeric values for the variables,  \n",
    "* A target variable (the 'answer' we're looking for).  \n",
    "\n",
    "The target variable is the class or category we want to detect, in this case whether or not a movie has won any awards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset \n",
    "import pandas as pd \n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/jargonautical/bsuBootcampCohort4/refs/heads/main/week3/netflix-rotten-tomatoes-metacritic-imdb.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "#df.head()\n",
    "#df.sample()\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column - if awards won > 0 then 1 else 0\n",
    "df['winner'] = df['Awards Received'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the numeric variables we will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to use only numeric values, so we supply a list of these\n",
    "trim_df = df.select_dtypes(include=['float64', 'int64']) \n",
    "trim_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df['Awards Received'] = trim_df['Awards Received'].fillna(0) \n",
    "trim_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim_df = trim_df.drop(columns = ['Awards Received'], axis=1)\n",
    "in_df = trim_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our model inputs X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = in_df.drop(columns = ['winner'], axis = 1)\n",
    "y = in_df['winner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model intercept: {}'.format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'.format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression 'herds' all the features and tries to work out which ones are significant\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state = 0)\n",
    "linridge = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "print('Properties dataset')\n",
    "print('ridge regression linear model intercept: {}'.format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'.format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=10.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Properties dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting max decision tree depth to help avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "# Putting the feature names and class names into variables\n",
    "fn = X.columns\n",
    "cn = ['winner', 'loser']\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "#fig.savefig('../images/plottreedefault.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "tree.plot_tree(clf2,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True,\n",
    "               fontsize=8\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, fn)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = in_df.values\n",
    "inertias = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_num_components = 2\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "in_df[\"kmeans_cluster\"] = kmeans.labels_\n",
    "\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(in_df.iloc[:,1:12])\n",
    "results = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\n",
    "\n",
    "sns.scatterplot(x=\"pca1\", y=\"pca2\", hue=in_df['kmeans_cluster'], data=results)\n",
    "plt.title('K-means Clustering with 2 dimensions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
